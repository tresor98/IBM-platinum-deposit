import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import KFold
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import RFE
from sklearn.feature_selection import SelectPercentile
from sklearn.dummy import DummyClassifier


df = pd.read_csv('data_20319681.csv')
#Getting rid of the column Unnamed: 0
df1=df.loc[:,'age':'y']

#Converting the target value to numeric
df1.loc[df1['y']=='yes','y']=1
df1.loc[df1['y']=='no','y']=0

#Removing these outliers from balance
df2 = df2.drop([288,4350,4830])

#Changing our categorical variables to dummy variables
df2_dummies = pd.get_dummies(df2)
print('Features after get_dummies: ',list(df2_dummies.columns))
print(len(list(df2_dummies.columns)))

#split my data into data and target
target = df2_dummies['y']
data = df2_dummies.drop(columns='y')

#Extract Numpy arrays
X = data
y = target


#Print the shape of our data and target
print(X.shape)
print(y.shape)


#Divide our data into training set and testing set
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)

#Using function to select only some features.
#We set the percentile of how many features we want to keep
select = SelectPercentile(percentile=30)
select.fit(X_train,y_train)
#get_support can show you which features have been chosen and which have been not
#select.get_support()
X_train_selected = select.transform(X_train)
X_test_selected = select.transform(X_test)

#standardize our training data and testing data
scaler = StandardScaler()
scaler.fit(X_train_selected)
X_train_scaled = scaler.transform(X_train_selected)
X_test_scaled = scaler.transform(X_test_selected)


#Using a model basedline 
dummy_clf = DummyClassifier(strategy="stratified")
#Train and test and display a confusion matrix
dummy = dummy_clf.fit(X_train_scaled,y_train)
pred = dummy.predict(X_test_scaled)
print('Confusion matrix ')
print(confusion_matrix(y_test,pred))
print()
#print a classification report that includes precision,recall and F-1
print('Logistic model classification report')
print(classification_report(y_test,pred))


#Using stratified k-fold cross validation
kfold = KFold(n_splits=10)

#Decision tree
#Applying cross validation
tree = DecisionTreeClassifier(max_depth=7, random_state=0)
scores = cross_val_score(tree,X_train_scaled,y_train,cv=kfold)
print('Scores from the decision tree ', scores)
print('Average validation score on decision tree is ',scores.mean())
#Training on the training set and testing on the testing set
#Display confusion matrix and classification report
print('Training and testing ')
tree1 = tree.fit(X_train_scaled,y_train)
pred_tree = tree1.predict(X_test_scaled)
print('Score on test set: ',tree1.score(X_test_scaled,y_test))
print('Confusion matrix ')
print(confusion_matrix(y_test,pred_tree))
print()
print('Classification report ')
print(classification_report(y_test,pred_tree))

















